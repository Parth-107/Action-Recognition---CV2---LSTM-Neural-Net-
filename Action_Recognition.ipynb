{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f3434f",
   "metadata": {},
   "source": [
    "# Install and import depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ae706",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflow-gpu opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20fa59a",
   "metadata": {},
   "source": [
    "# Keypoints using Mediapipe Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d06936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic     # Holistic model\n",
    "mp_drwing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model):\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)   # Convert the color from BGR to RGB\n",
    "    image.flags.writeable = False                  # Set image to be not writeable\n",
    "    result = model.process(image)                  # Make the prediction\n",
    "    image.flags.writeable = True                   # Set image back to writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # Convert the color back from RGB to BGR\n",
    "    \n",
    "    return image, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2106a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, result):\n",
    "    # Draw the Face connections\n",
    "    mp_drwing.draw_landmarks(img, result.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drwing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drwing.DrawingSpec(color=(100,256,121), thickness=1, circle_radius=1))\n",
    "    \n",
    "    # Draw the Pose connections\n",
    "    mp_drwing.draw_landmarks(img, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drwing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=3),\n",
    "                             mp_drwing.DrawingSpec(color=(80,44,10), thickness=2, circle_radius=2))\n",
    "    \n",
    "    # Draw the Hand connections\n",
    "    mp_drwing.draw_landmarks(img, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drwing.DrawingSpec(color=(121,22,10), thickness=2, circle_radius=3),\n",
    "                             mp_drwing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)) \n",
    "    \n",
    "    # Draw the Hand connections\n",
    "    mp_drwing.draw_landmarks(img, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drwing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=3),\n",
    "                             mp_drwing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        #Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make the detection\n",
    "        image, result = mediapipe_detection(frame, model=holistic)\n",
    "        #print(result)\n",
    "        \n",
    "        #Draw Landmarks in real time\n",
    "        draw_landmarks(image, result)\n",
    "        \n",
    "        cv2.imshow(\"OpenCV Feed\", image)\n",
    "        #Break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c204f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, result) #Call the draw_landmarks function and passing the frame and result\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) #Show the landmarks on the static image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35fe6ec",
   "metadata": {},
   "source": [
    "# Extract Keypoints values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad2cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680debab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create teh function for above keypoints that we stored\n",
    "def extract_keypoints(result):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(132)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(1404)\n",
    "    left_hand = np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hand = np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    # Concatenate all the above results\n",
    "    return np.concatenate([pose, face, right_hand, left_hand], axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beaef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the function\n",
    "result_test = extract_keypoints(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('0',result_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee33973c",
   "metadata": {},
   "source": [
    "# Set up folders for collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7720766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for exported data, numpy array\n",
    "data_path = os.path.join('MP_Data')\n",
    "\n",
    "# Action trying to Detect\n",
    "actions = np.array(['Hello', 'Thanks', 'Iloveyou'])\n",
    "\n",
    "# Make 30 videos worth data\n",
    "no_sequence = 30\n",
    "\n",
    "# Videos length with 30 frames\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef10642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for each actions. In our case it is \"Hello\", \"Thanks\", \"Iloveyou\"\n",
    "for action in actions:\n",
    "    #within each sub folders we have 0 to 29 folders for no of sequence\n",
    "    for sequence in range(no_sequence):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(data_path, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23d2ca",
   "metadata": {},
   "source": [
    "# Collecting keypoints values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "   \n",
    "    # Loop through all the actions\n",
    "    for action in actions:\n",
    "        # Loop through each of the video (here 3o times)\n",
    "        for sequence in range(no_sequence):\n",
    "            # Loop through Video length\n",
    "            for frame_num in range(sequence_length):\n",
    "    \n",
    "    \n",
    "                #Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make the detection\n",
    "                image, result = mediapipe_detection(frame, model=holistic)\n",
    "                #print(result)\n",
    "\n",
    "                #Draw Landmarks in real time\n",
    "                draw_landmarks(image, result)\n",
    "                \n",
    "                # Apply wait logic\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'Starting Collection', (120,200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting Frames for {} video number {}'.format(action, sequence), (15,12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow(\"OpenCV Feed\", image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting Frames for {} video number {}'.format(action, sequence), (15,12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow(\"OpenCV Feed\", image)\n",
    "                \n",
    "                # Export Keyt points\n",
    "                keypoints = extract_keypoints(result)\n",
    "                npy_path = os.path.join(data_path, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                #Break\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a6f78",
   "metadata": {},
   "source": [
    "# Preprocess Data and create Lable and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7aed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Dictionary for the actions (Hello = 0, Thanks = 1, Iloveyou = 3)\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c0827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences represents the X data and labels represents the y data. (features and labels)\n",
    "sequences, labels = [], []\n",
    "\n",
    "# Loop through each of the action (Hello, Thanks, and Iloveyou)\n",
    "for action in actions:\n",
    "    # Loop through each of the sequences, remember we have 30 no_sequence\n",
    "    for sequence in range(no_sequence):\n",
    "        window = []\n",
    "        # Loop through each of the video frames (in each video 30 frames of length).\n",
    "        for frame_num in range(sequence_length):\n",
    "            # Load the numpy frames that we have stored\n",
    "            res = np.load(os.path.join(data_path, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        # Once Video1 is done\n",
    "        \n",
    "        # Append to the sequences array\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Features\n",
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the One-hot encoding\n",
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c8a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply the Train and testing split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95cdd16",
   "metadata": {},
   "source": [
    "# Build and Train LSTM Nueral Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TensorBoard callback to store the Log of Model\n",
    "#log_dir = os.path.join('Logs')\n",
    "#tb_callback = TensorBoard(log_dir = log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30, 1662))) # X shape is (90, 30, 1662). Input layer\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu')) # return_sequences=False because next layer is not the LSTM\n",
    "model.add(Dense(64, activation='relu')) # Dense layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax')) # Output layer (action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87925dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer= 'Adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186673d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs= 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the Summary of the Model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a65f7a",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e63f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(pred[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1151a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the Predictions with the test data\n",
    "\n",
    "# Prediction\n",
    "actions[np.argmax(pred[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "actions[np.argmax(y_test[3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60395cb9",
   "metadata": {},
   "source": [
    "# Save Weights of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4959ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the Model\n",
    "#model.load_weights('Action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e4fa9",
   "metadata": {},
   "source": [
    "# Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc66e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59902cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Accuracy Score\n",
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d268f",
   "metadata": {},
   "source": [
    "# Test in  Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the Detection Variable\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.7\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        #Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make the detection\n",
    "        image, result = mediapipe_detection(frame, model=holistic)\n",
    "        #print(result)\n",
    "        \n",
    "        #Draw Landmarks in real time\n",
    "        draw_landmarks(image, result)\n",
    "        \n",
    "        # 2. Prediction Logic\n",
    "        keypints = extract_keypoints(result)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            # Our model's Prediction shape is (30,1662). But, we want the shape (num_sequences (1), 30, 1662). Thus we need to\n",
    "            # use np.expand_dims function.\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0] \n",
    "            print(actions[np.argmax(res)])\n",
    "        \n",
    "            # Vizulization logic\n",
    "            if res[np.argmax(res)] > threshold:\n",
    "                if len(sentence) > 0:\n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "        \n",
    "        # Draw on the Screen\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow(\"OpenCV Feed\", image)\n",
    "        #Break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7877ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557a759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_venv",
   "language": "python",
   "name": "ai_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
